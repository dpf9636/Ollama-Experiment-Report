# 大模型基础实验报告：Ollama 本地部署与 API 调用实战

## 1. 实验环境说明

### 1.1 硬件配置
* **部署平台:** 本地部署
* **操作系统:**操作系统名称	Microsoft Windows 11 家庭中文版
* **硬件配置:**
    * **CPU:** 处理器	AMD Ryzen 7 7735H with Radeon Graphics，3201 Mhz，8 个内核，16 个逻辑处理器
    * **GPU:** RTX4060LAPTOP GPU
    * **显存大小 (VRAM):** 16GB

### 2. 核心代码说明
```import ollama
import time
import json
from typing import Dict, Any, Generator

# --------------------------
# 配置部分：使用您已安装的模型
# --------------------------
MODEL_NAME = "qwen2.5:7b" 
OLLAMA_HOST = 'http://localhost:11434' 

try:
    # 尝试连接 Ollama 客户端
    client = ollama.Client(host=OLLAMA_HOST)
    print(f"✅ Ollama 客户端连接成功，模型: {MODEL_NAME}")
except Exception as e:
    print(f"❌ 警告：Ollama 客户端连接失败，请确保服务正在运行。错误: {e}")
    # 如果连接失败，后续代码可能无法运行

# --------------------------
# 任务 1 & 4: 问答系统与推理速度记录
# --------------------------
def run_simple_qa(prompt: str, model: str = MODEL_NAME) -> str:
    """实现简单问答并记录推理速度。"""
    print(f"\n--- 任务 1 & 4: 运行问答系统并记录速度 ({model}) ---")
    
    start_time = time.time()
    
    try:
        # 使用 client.generate 进行调用
        response = client.generate(
            model=model,
            prompt=prompt,
            options={
                "temperature": 0.5, # 默认参数用于基准测试
                "top_p": 0.9
            }
        )
        
        end_time = time.time()
        total_time = end_time - start_time
        
        output_text = response.get('response', '响应为空')
        
        # 记录 tokens/s
        if 'eval_count' in response and 'eval_duration' in response:
            eval_count = response['eval_count']
            eval_duration_s = response['eval_duration'] / 1e9 # 纳秒转秒
            
            if eval_duration_s > 0:
                tokens_per_second = eval_count / eval_duration_s
                print(f"--- 性能数据 ({model}) ---")
                print(f"总生成 tokens: {eval_count}")
                print(f"总耗时 (含网络): {total_time:.2f} 秒")
                print(f"纯生成耗时: {eval_duration_s:.2f} 秒")
                print(f"**推理速度 (tokens/s): {tokens_per_second:.2f}**")
            else:
                print("未能获取有效的生成耗时信息。")

        print("\n[模型响应]:")
        # 打印响应的前几行
        print('\n'.join(output_text.splitlines()[:10]))
        print("...")
        return output_text
        
    except Exception as e:
        print(f"API 调用失败: {e}")
        return ""

# --------------------------
# 任务 2: 测试不同 Temperature 和 Top-p 参数
# --------------------------
def test_parameters(prompt: str, params_list: list[Dict[str, Any]], model: str = MODEL_NAME):
    """测试不同参数配置对输出的影响。"""
    print(f"\n--- 任务 2: 参数调优测试 ({model}) ---")
    
    for i, params in enumerate(params_list):
        temp = params.get("temperature", 0.8)
        top_p = params.get("top_p", 0.9)
        
        print(f"\n>>> 测试 {i+1}: Temperature={temp}, Top_P={top_p} <<<")
        
        try:
            response = client.generate(
                model=model,
                prompt=prompt,
                options={
                    "temperature": temp,
                    "top_p": top_p
                }
            )
            
            output_text = response.get('response', '响应为空').strip()
            
            print("[模型输出片段 (前 3 行)]: 模型输出的创造性将影响您的报告分析。")
            print("\n".join(output_text.splitlines()[:3]))
            print("...") 

        except Exception as e:
            print(f"调用失败: {e}")

# --------------------------
# 任务 3: 实现流式输出 (Streaming) 功能
# --------------------------
def run_streaming_qa(prompt: str, model: str = MODEL_NAME):
    """实现流式输出功能。"""
    print(f"\n--- 任务 3: 实现流式输出 ({model}) ---")
    print("[模型流式响应开始]:")
    
    try:
        # 核心：设置 stream=True
        stream_response: Generator[Dict[str, Any], None, None] = client.generate(
            model=model,
            prompt=prompt,
            stream=True,  
        )
        
        # 迭代流式响应，逐块打印，实现流式效果
        for chunk in stream_response:
            if 'response' in chunk:
                # flush=True 确保立即显示到终端
                print(chunk['response'], end="", flush=True)
        
        print("\n[流式输出完成]")

    except Exception as e:
        print(f"\n流式调用失败: {e}")


# --------------------------
# 主执行逻辑!
# --------------------------
if __name__ == "__main__":
    
    # 问答和流式输出的提示词
    qa_prompt = "请用中文简述大模型的Transformer架构核心思想，并说明其优势和局限性。"
    # 用于测试参数的更有创造性的提示词
    creative_prompt = "如果你是一只生活在未来赛博朋克世界的猫，请写一段关于你一天生活的独白，重点描述你如何与智能机器人交流。"

    # 1. 运行任务 1 & 4
    run_simple_qa(qa_prompt)

    # 2. 运行任务 2: 参数调优
    test_configs = [
        {"temperature": 0.1, "top_p": 0.9}, # 低 T (保守、确定性高)
        {"temperature": 0.8, "top_p": 0.9}, # 均衡 T (默认)
        {"temperature": 1.5, "top_p": 0.8}, # 高 T (随机、创造性高)
    ]
    test_parameters(creative_prompt, test_configs)

    # 3. 运行任务 3
    run_streaming_qa(qa_prompt)

    print("\n--- 初步实验任务运行完毕。请继续进行性能对比步骤。---")
```
### 3. 运行结果与推理速度对比

| 模型版本 | 总生成 Tokens | 纯生成耗时 (s) | **推理速度 (tokens/s)** |
| :--- | :---: | :---: | :---: |
| qwen2.5:1.5b | 375 | 2.30 | **163.12** |
| qwen2.5:7b | 456 | 11.31 | **40.31** |



![enter image description here](https://github.com/dpf9636/Ollama-Experiment-Report/blob/80b9ca04481c0e4f0c896c2c4e65c83783f7c5ba/1.png)
![enter image description here](https://github.com/dpf9636/Ollama-Experiment-Report/blob/80b9ca04481c0e4f0c896c2c4e65c83783f7c5ba/2.png)
![enter image description here](https://github.com/dpf9636/Ollama-Experiment-Report/blob/80b9ca04481c0e4f0c896c2c4e65c83783f7c5ba/3.png)
