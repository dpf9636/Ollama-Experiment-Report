# 大模型基础实验报告：Ollama 本地部署与 API 调用实战

## 1. 实验环境说明

* **部署平台:** 本地部署
* **操作系统:**操作系统名称	Microsoft Windows 11 家庭中文版
* **硬件配置:**
    * **CPU:** 处理器	AMD Ryzen 7 7735H with Radeon Graphics，3201 Mhz，8 个内核，16 个逻辑处理器
    * **GPU:** RTX4060LAPTOP GPU
    * **显存大小 (VRAM):** 16GB

## 2. 实验任务一 & 四：问答系统与推理速度对比

### 2.1 任务目标
实现一个基础问答系统，并对比同款基座模型（Qwen 2.5）不同参数规模版本间的推理速度 (`tokens/s`) 和运行时间。

### 2.2 核心代码与测试提示词
本实验使用 `ollama.Client().generate` 方法进行同步调用，并通过解析响应中的 `eval_count` 和 `eval_duration` 计算推理速度。

**测试提示词:** "请用中文简述大模型的Transformer架构核心思想，并说明其优势和局限性。"

### 2.3核心代码说明
```import ollama
import time
import json
from typing import Dict, Any, Generator

# --------------------------
# 配置部分：使用您已安装的模型
# --------------------------
MODEL_NAME = "qwen2.5:7b" 
OLLAMA_HOST = 'http://localhost:11434' 

try:
    # 尝试连接 Ollama 客户端
    client = ollama.Client(host=OLLAMA_HOST)
    print(f"✅ Ollama 客户端连接成功，模型: {MODEL_NAME}")
except Exception as e:
    print(f"❌ 警告：Ollama 客户端连接失败，请确保服务正在运行。错误: {e}")
    # 如果连接失败，后续代码可能无法运行

# --------------------------
# 任务 1 & 4: 问答系统与推理速度记录
# --------------------------
def run_simple_qa(prompt: str, model: str = MODEL_NAME) -> str:
    """实现简单问答并记录推理速度。"""
    print(f"\n--- 任务 1 & 4: 运行问答系统并记录速度 ({model}) ---")
    
    start_time = time.time()
    
    try:
        # 使用 client.generate 进行调用
        response = client.generate(
            model=model,
            prompt=prompt,
            options={
                "temperature": 0.5, # 默认参数用于基准测试
                "top_p": 0.9
            }
        )
        
        end_time = time.time()
        total_time = end_time - start_time
        
        output_text = response.get('response', '响应为空')
        
        # 记录 tokens/s
        if 'eval_count' in response and 'eval_duration' in response:
            eval_count = response['eval_count']
            eval_duration_s = response['eval_duration'] / 1e9 # 纳秒转秒
            
            if eval_duration_s > 0:
                tokens_per_second = eval_count / eval_duration_s
                print(f"--- 性能数据 ({model}) ---")
                print(f"总生成 tokens: {eval_count}")
                print(f"总耗时 (含网络): {total_time:.2f} 秒")
                print(f"纯生成耗时: {eval_duration_s:.2f} 秒")
                print(f"**推理速度 (tokens/s): {tokens_per_second:.2f}**")
            else:
                print("未能获取有效的生成耗时信息。")

        print("\n[模型响应]:")
        # 打印响应的前几行
        print('\n'.join(output_text.splitlines()[:10]))
        print("...")
        return output_text
        
    except Exception as e:
        print(f"API 调用失败: {e}")
        return ""

# --------------------------
# 任务 2: 测试不同 Temperature 和 Top-p 参数
# --------------------------
def test_parameters(prompt: str, params_list: list[Dict[str, Any]], model: str = MODEL_NAME):
    """测试不同参数配置对输出的影响。"""
    print(f"\n--- 任务 2: 参数调优测试 ({model}) ---")
    
    for i, params in enumerate(params_list):
        temp = params.get("temperature", 0.8)
        top_p = params.get("top_p", 0.9)
        
        print(f"\n>>> 测试 {i+1}: Temperature={temp}, Top_P={top_p} <<<")
        
        try:
            response = client.generate(
                model=model,
                prompt=prompt,
                options={
                    "temperature": temp,
                    "top_p": top_p
                }
            )
            
            output_text = response.get('response', '响应为空').strip()
            
            print("[模型输出片段 (前 3 行)]: 模型输出的创造性将影响您的报告分析。")
            print("\n".join(output_text.splitlines()[:3]))
            print("...") 

        except Exception as e:
            print(f"调用失败: {e}")

# --------------------------
# 任务 3: 实现流式输出 (Streaming) 功能
# --------------------------
def run_streaming_qa(prompt: str, model: str = MODEL_NAME):
    """实现流式输出功能。"""
    print(f"\n--- 任务 3: 实现流式输出 ({model}) ---")
    print("[模型流式响应开始]:")
    
    try:
        # 核心：设置 stream=True
        stream_response: Generator[Dict[str, Any], None, None] = client.generate(
            model=model,
            prompt=prompt,
            stream=True,  
        )
        
        # 迭代流式响应，逐块打印，实现流式效果
        for chunk in stream_response:
            if 'response' in chunk:
                # flush=True 确保立即显示到终端
                print(chunk['response'], end="", flush=True)
        
        print("\n[流式输出完成]")

    except Exception as e:
        print(f"\n流式调用失败: {e}")


# --------------------------
# 主执行逻辑!
# --------------------------
if __name__ == "__main__":
    
    # 问答和流式输出的提示词
    qa_prompt = "请用中文简述大模型的Transformer架构核心思想，并说明其优势和局限性。"
    # 用于测试参数的更有创造性的提示词
    creative_prompt = "如果你是一只生活在未来赛博朋克世界的猫，请写一段关于你一天生活的独白，重点描述你如何与智能机器人交流。"

    # 1. 运行任务 1 & 4
    run_simple_qa(qa_prompt)

    # 2. 运行任务 2: 参数调优
    test_configs = [
        {"temperature": 0.1, "top_p": 0.9}, # 低 T (保守、确定性高)
        {"temperature": 0.8, "top_p": 0.9}, # 均衡 T (默认)
        {"temperature": 1.5, "top_p": 0.8}, # 高 T (随机、创造性高)
    ]
    test_parameters(creative_prompt, test_configs)

    # 3. 运行任务 3
    run_streaming_qa(qa_prompt)

    print("\n--- 初步实验任务运行完毕。请继续进行性能对比步骤。---")
```

### 2.4 运行结果与推理速度对比

| 模型版本 | 总生成 Tokens | 纯生成耗时 (s) | **推理速度 (tokens/s)** |
| :--- | :---: | :---: | :---: |
| qwen2.5:1.5b | 375 | 2.30 | **163.12** |
| qwen2.5:7b | 456 | 11.31 | **40.31** |

### 2.5 性能对比分析


* 尽管 `qwen2.5:7b` (456 Tokens) 生成的文本量比 `qwen2.5:1.5b` (375 Tokens) 略多，但在纯生成耗时上慢了约 **[填写 11.31 / 2.30 结果]** 倍，导致推理速度从 **163.12 tokens/s** 急剧下降到 **40.31 tokens/s**。
* **主要原因：** 参数规模的增加（5倍）导致每一步矩阵运算量暴增，且 **[推测原因：例如显存 VRAM 接近饱和，或计算核心/内存带宽成为瓶颈]**，从而验证了模型规模是影响推理速度的决定性因素。

## 3. 实验任务二：测试不同 Temperature 和 Top-p 参数

### 3.1 任务目标
测试不同的 `Temperature` (T) 和 `Top-p` 对模型输出的随机性和创造性的影响。
**测试模型:** `qwen2.5:7b`
**测试提示词:** "如果你是一只生活在未来赛博朋克世界的猫，请写一段关于你一天生活的独白，重点描述你如何与智能机器人交流。"

### 3.2 运行结果对比

| 配置 | Temperature | Top-p | 模型输出片段特征 |
| :--- | :---: | :---: | :--- |
| **配置 1 (保守)** | 0.1 | 0.9 | [输出逻辑性强，词汇选择确定性高，命名（如“墨流苏”）较为传统。] |
| **配置 2 (均衡)** | 0.8 | 0.9 | [输出平衡了事实与创造性，出现了更丰富的智能设备描述（如“智能镜”）。] |
| **配置 3 (激进)** | 1.5 | 0.8 | [输出最具随机性和创造性，出现了独特的、更具赛博朋克风格的名称（如“零号”、“米斯”），但叙事可能略显跳跃。] |

### 3.3 参数调优心得体会

**[请在此处填写心得体会，重点说明：]**

* **Temperature (T)：** **[填写：T 值越低，模型越“保守”，输出越接近训练数据的标准答案；T 值越高，模型越“创造性”，但可能牺牲连贯性。]**
* **Top-p：** 即使 T 值很高（如 1.5），Top-p 仍能通过限制模型只从概率最高的词汇中采样，从而在一定程度上**控制输出的离散度**，防止其完全失控。

## 4. 实验任务三：实现流式输出 (Streaming) 功能

### 4.1 任务目标
通过设置 `stream=True` 实现模型的流式输出，模拟实时对话体验。

### 4.2 实现说明
通过 Python `ollama.Client().generate(..., stream=True)` 方法实现。程序通过循环迭代响应块，并使用 `end=""` 和 `flush=True` 逐个打印 Token。

## 5.运行结果截图
此处为qwen2.5:7b的截图
![enter image description here](https://github.com/dpf9636/Ollama-Experiment-Report/blob/80b9ca04481c0e4f0c896c2c4e65c83783f7c5ba/1.png)
![enter image description here](https://github.com/dpf9636/Ollama-Experiment-Report/blob/80b9ca04481c0e4f0c896c2c4e65c83783f7c5ba/2.png)
![enter image description here](https://github.com/dpf9636/Ollama-Experiment-Report/blob/80b9ca04481c0e4f0c896c2c4e65c83783f7c5ba/3.png)
此处为qwen2.5:1.5b的截图
![enter image description here](https://github.com/dpf9636/Ollama-Experiment-Report/blob/80b9ca04481c0e4f0c896c2c4e65c83783f7c5ba/4.png)
![enter image description here](https://github.com/dpf9636/Ollama-Experiment-Report/blob/80b9ca04481c0e4f0c896c2c4e65c83783f7c5ba/5.png)
![enter image description here](https://github.com/dpf9636/Ollama-Experiment-Report/blob/80b9ca04481c0e4f0c896c2c4e65c83783f7c5ba/6.png)
![enter image description here](https://github.com/dpf9636/Ollama-Experiment-Report/blob/80b9ca04481c0e4f0c896c2c4e65c83783f7c5ba/7.png)

## 6. 遇到的问题及解决方案

| 遇到的问题 | 解决方案 |
| :--- | :--- |
| **`ollama pull qwen2.5:7b` 速度极慢，进度卡顿（4GB的文件下了我一个半小时，气晕了）** | 意识到可能是**文件校验**和**磁盘 I/O** 成为瓶颈，保持耐心等待，避免中断导致文件损坏。 |
| **前面用Spyder写的，没想到一直502报错，折腾了一个多小时** | 直接换python |
| **代码不会写** | gemini来帮忙 |
